{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b63c357",
   "metadata": {},
   "source": [
    "## Task : Maximize the score in defend the centre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6535c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "from vizdoom import DoomGame\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from gymnasium import Env\n",
    "\n",
    "\n",
    "from gymnasium.spaces import Box,Discrete\n",
    "\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from stable_baselines3.common import env_checker\n",
    "\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "CHECK_POINT_DIR=\"./train/Centre\"\n",
    "LOG_DIR=\"./log/Centre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f34758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "            if self.verbose:\n",
    "                print(f\" Model checkpoint saved at step {self.n_calls} â†’ {model_path}\")\n",
    "        return True\n",
    "    \n",
    "callback= TrainAndLoggingCallback(check_freq=10000,save_path=CHECK_POINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58947831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomEnv(Env):\n",
    "    def __init__ (self,render=False,config=\"github/ViZDoom/scenarios/defend_the_center.cfg\",tics=2,punishment=-0.1):\n",
    "        super().__init__()\n",
    "        self.game=DoomGame()\n",
    "        self.game.load_config(config)\n",
    "        self.game.set_window_visible(render)\n",
    "\n",
    "        self.game.init()\n",
    "        self.tics=tics\n",
    "        self.punish=punishment\n",
    "\n",
    "        self.observation_space=Box(low=0,high=255,shape=(100,160,1), dtype=np.uint8)\n",
    "        self.action_space=Discrete(3)\n",
    "\n",
    "\n",
    "        # self.game.set_damage_taken_penalty(-0.01)\n",
    "        # self.game.set_living_reward(0.01)\n",
    "    def step(self,action):\n",
    "            actions = np.identity(3, dtype=np.uint8)\n",
    "            state_obj = self.game.get_state()\n",
    "            if state_obj:\n",
    "                prev_health = state_obj.game_variables[1]  #  2nd variable is HEALTH\n",
    "                prev_ammo = state_obj.game_variables[0]    # AMMO2\n",
    "            else:\n",
    "                prev_health = 100\n",
    "        \n",
    "                prev_ammo = 26  # default ammo\n",
    "\n",
    "            reward =0\n",
    "\n",
    "            if action == 2:  # Shoot\n",
    "                reward +=self.punish # Penalty for shooting\n",
    "            \n",
    "\n",
    "            action_reward = self.game.make_action(actions[action], self.tics)\n",
    "            reward += action_reward\n",
    "\n",
    "             # Get new state after action\n",
    "            if self.game.get_state():\n",
    "                new_vars = self.game.get_state().game_variables\n",
    "                new_ammo = new_vars[0]\n",
    "                new_health = new_vars[1]\n",
    "            else:\n",
    "                new_health =prev_health\n",
    "                new_ammo = prev_ammo\n",
    "\n",
    "\n",
    "            reward += (new_health - prev_health) * 0.05  # Reward for health change\n",
    "\n",
    "            terminated = self.game.is_episode_finished()\n",
    "            truncated = False  # \n",
    "            if not terminated:\n",
    "                reward += 0.05  # Small reward for staying alive\n",
    "\n",
    "            if self.game.get_state():\n",
    "                state = self.game.get_state().screen_buffer\n",
    "                state = self.greyscale(state)\n",
    "                ammo = self.game.get_state().game_variables[0]\n",
    "                info = {\"ammo\": ammo}\n",
    "            else:\n",
    "                state = np.zeros(self.observation_space.shape, dtype=np.uint8)\n",
    "                info = {\"ammo\": 0}\n",
    "\n",
    "            return state, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "    def greyscale(self,observation):\n",
    "        grey= cv2.cvtColor(np.moveaxis(observation,0,-1),cv2.COLOR_BGR2GRAY) #move axis to reoder the channels to (240,320,3)\n",
    "        resize=cv2.resize(grey,(160,100),interpolation=cv2.INTER_CUBIC)\n",
    "        state=np.reshape(resize,(100,160,1))\n",
    "        return state\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)  # Let Gymnasium handle seeding\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)  # Optional, for reproducibility\n",
    "\n",
    "        self.game.new_episode()\n",
    "        observation = self.greyscale(self.game.get_state().screen_buffer)\n",
    "\n",
    "        info = {}  # You can add useful debug info here if needed\n",
    "        return observation, info\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3acf9e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=VizDoomEnv(render=True)\n",
    "env_checker.check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=VizDoomEnv(render=True)\n",
    "\n",
    "while True:\n",
    "    state,reward,terminated,truncated,info=env.step(0)\n",
    "    print(reward)\n",
    "    time.sleep(0.05)\n",
    "    if terminated or truncated: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b74f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = make_vec_env(lambda: VizDoomEnv(render=False, punishment=0, tics=3), n_envs=4)\n",
    "\n",
    "model = PPO(\n",
    "    \"CnnPolicy\",\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=LOG_DIR,\n",
    "    n_steps=2048,\n",
    "    batch_size=256,\n",
    "    learning_rate=1e-5,\n",
    "    callback=callback\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1636c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import time\n",
    "\n",
    "def play(model_path=\"train/Centre/best_model_250000.zip\", render=True, num_episodes=5, sleep_time=0.03):\n",
    "    \"\"\"\n",
    "    Run a trained PPO agent on ViZDoom for visualization.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the saved PPO model.\n",
    "        render (bool): Whether to enable rendering.\n",
    "        num_episodes (int): How many episodes to run.\n",
    "        sleep_time (float): Time to wait between frames (for FPS control).\n",
    "    \"\"\"\n",
    "    # Import your custom ViZDoom environment class\n",
    " # <- change this to your actual file\n",
    "\n",
    "    # Create a single environment (non-vectorized for inference)\n",
    "    def make_env():\n",
    "        return VizDoomEnv(render=render, punishment=0, tics=3)\n",
    "\n",
    "    env = DummyVecEnv([make_env])  # Wrap to match training format\n",
    "\n",
    "    # Load trained model\n",
    "    model = PPO.load(model_path)\n",
    "\n",
    "    # Run episodes\n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward[0]  # Since DummyVecEnv returns a list\n",
    "            step += 1\n",
    "\n",
    "            if render:\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "        print(f\"Episode {ep+1} finished in {step} steps with total reward: {total_reward:.2f}\")\n",
    "\n",
    "    env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doom_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
